{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0b74d6",
   "metadata": {},
   "source": [
    "# Workshop: Two-Agent Health Checker on AMD GPUs (vLLM + MCP)\n",
    "\n",
    "In this hands-on, you’ll build a **two-model, two-agent system** designed to answer questions like:\n",
    "\n",
    "> **“Is this snack OK for someone with high blood pressure?”**\n",
    "\n",
    "The system combines a conversational **Orchestrator**, which browses for up-to-date ingredient information, with a focused **Hypertension Consultant** that evaluates the ingredient list and delivers a clear judgment for someone watching their blood pressure: OK, Caution, or Avoid.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc1c8f",
   "metadata": {},
   "source": [
    "![Arch overview](./multi-agent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2275be",
   "metadata": {},
   "source": [
    "## What you’ll build\n",
    "\n",
    "- **Orchestrator Agent** (Qwen3-30B-A3B-Instruct-2507): Engages in natural conversation, performs **mandatory browsing** via MCP/Exa to gather up-to-date ingredient data, extracts a clean ingredient list, and then calls the appropriate tool.\n",
    "- **Consultant Agent** (GPT-OSS-120B): Analyzes an ingredient list specifically for high blood pressure concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ac9f0",
   "metadata": {},
   "source": [
    "## What you’ll learn\n",
    "\n",
    "- Serving two large open models with **vLLM** on **AMD ROCm**.\n",
    "- Building agents and tools using **Pydantic AI**.\n",
    "- Integrating **MCP** browsing through Exa to enable agents to fetch real-time facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566c5a7",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "1. Install dependencies and serve **both model endpoints on one AMD GPU**.\n",
    "1. Build the **Consultant** Agent (GPT-OSS-120B): Create a JSON-only verdict generator (no browsing) that evaluates ingredients for high blood pressure risks.\n",
    "1. Wrap the Consultant as a tool: Implement `consult_hypertension` to analyze ingredient lists programmatically.\n",
    "1. Build the **Orchestrator** (Qwen3-30B-A3B-Instruct-2507): Set the core intent —\n",
    "“Always call web_search_exa(query) to retrieve current ingredient data; never rely on memory alone.”\n",
    "1. End-to-end runs: Analyze real-world examples such as KitKat (US)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524b8c5",
   "metadata": {},
   "source": [
    "### Step 0: Start model endpoints (run in separate terminals before the notebook)\n",
    "\n",
    "Before running the notebook, you need to start both model endpoints in separate terminals. This ensures the Orchestrator and Consultant agents are available for the rest of the workshop.\n",
    "\n",
    "#### Consultant (GPT-OSS-120B) on port 9000\n",
    "First, start the GPT-OSS-120B for the consultant.\n",
    "\n",
    "**Note**: The following command needs to be run in a separate terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8b06d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "# Consultant (GPT-OSS-120B) on :9000\n",
    "vllm serve /models/gpt-oss-120b \\\n",
    "  --tensor-parallel 1 \\\n",
    "  --no-enable-prefix-caching \\\n",
    "  --port 9000 \\\n",
    "  --compilation-config '{\"full_cuda_graph\": true}' \\\n",
    "  --gpu-memory_utilization 0.5\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54fe2d",
   "metadata": {},
   "source": [
    "#### Orchestrator (Qwen/Qwen3-30B-A3B-Instruct-2507) on port 9001\n",
    "Once GPT-OSS has fully started, launch the Qwen/Qwen3-30B-A3B-Instruct-2507 model to run the Orchestrator.\n",
    "\n",
    "**Note**: Starting both models at the same time may cause out-of-memory errors. To avoid this, wait for the first model to finish loading before starting the second. The following command also needs to be run in a separate terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd3746",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "# Orchestrator (Qwen3-30B-A3B-Instruct-2507) on :9001\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507-FP8 \\\n",
    "  --served-model-name /models/Qwen3-30B-A3B-Instruct-2507 \\\n",
    "  --port 9001 \\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser hermes \\\n",
    "  --trust-remote-code \\\n",
    "  --gpu-memory_utilization 0.45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c696868",
   "metadata": {},
   "source": [
    "### Step 1: Test Model Endpoints\n",
    "Let’s verify that both endpoints are running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9bfdb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Test the Consultant (GPT-OSS-120B) endpoint\n",
    "!curl http://localhost:9000/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740ef97",
   "metadata": {},
   "source": [
    "Great! Now let's begin building our agentic health checker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbef2d3",
   "metadata": {},
   "source": [
    "### Step 2: Install Dependencies\n",
    "\n",
    "Install the PydanticAI dependencies using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pydantic_ai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68233d1",
   "metadata": {},
   "source": [
    "### Step 3: Consultant Agent Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc8ca3",
   "metadata": {},
   "source": [
    "Now, let’s define the Consultant agent with GPT-OSS-120B model you started earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:9000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    http_client=httpx.Client(http2=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765c39d",
   "metadata": {},
   "source": [
    "This agent will analyze ingredient lists and return a JSON response. Let's give it a try by giving it a simple list of \"tofu, soy sauce, oil, sugar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d346b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.responses.create(\n",
    "    model=\"/models/gpt-oss-120b\",\n",
    "    input=\"Analyze these ingredients and return a JSON response: tofu, soy sauce, oil, sugar\"\n",
    ")\n",
    "\n",
    "# Find only assistant messages and print their text\n",
    "for item in resp.output:\n",
    "    if item.type == \"message\":\n",
    "        for c in item.content:\n",
    "            if c.type == \"output_text\":\n",
    "                print(c.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0e193",
   "metadata": {},
   "source": [
    "### Step 4: Build the Consultant Agent with Pydantic AI\n",
    "\n",
    "Now, let's use Pydantic AI to turn the Consultant model into an agent to do this task. By defining a system prompt, we can guide the model to act as the consultant we need analyzing ingredients for high blood pressure risks and returning the results as structured JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a597f-2f41-4224-b36e-5e82f5944e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "provider = OpenAIProvider(base_url=\"http://localhost:9000/v1\", api_key=\"EMPTY\")\n",
    "consultant_model = OpenAIChatModel(\"/models/gpt-oss-120b\", provider=provider)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a Hypertension Food Consultant.\n",
    "Your job is to read an ingredient list (raw text or JSON) and decide whether the item\n",
    "should be avoided, used with caution, or is generally OK for someone with high blood pressure.\n",
    "Use your general nutrition knowledge only. Be brief, neutral, and safety-first.\n",
    "If information is insufficient, say so and lean conservative.\n",
    "\n",
    "Return JSON only:\n",
    "{\n",
    "  \"avoid\": [ {\"name\":\"string\",\"reason\":\"≤12 words\"} ],\n",
    "  \"caution\": [ {\"name\":\"string\",\"reason\":\"≤12 words\"} ],\n",
    "  \"overall\": \"avoid|caution|ok\",\n",
    "  \"notes\": [\"optional short tip\"],\n",
    "  \"disclaimer\": \"Educational only; not medical advice.\"\n",
    "}\n",
    "\n",
    "Rules of output:\n",
    "- Extract and normalize ingredients from whatever text is provided.\n",
    "- List only ingredients actually present.\n",
    "- Keep reasons short and factual.\n",
    "- Choose overall based on your judgment, prioritizing safety.\n",
    "\"\"\"\n",
    "\n",
    "consultant_agent = Agent(\n",
    "    model=consultant_model,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50206fd8",
   "metadata": {},
   "source": [
    "### Step 5: Run the Consultant Agent\n",
    "Test the Consultant agent with another sample ingredient list with some less familiar ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48dcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_async(prompt: str) -> str:\n",
    "    async with consultant_agent:\n",
    "        result = await consultant_agent.run(prompt)\n",
    "        return result.output\n",
    "\n",
    "await run_async(\"Ingredients: Glucose syrup, Dipotassium glycyrrhizate (E958), Natural flavors, Caramel color.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af321b3f",
   "metadata": {},
   "source": [
    "### Step 6: Orchestrator (Phase 1) – Fetch Ingredients\n",
    "\n",
    "Begin by configuring the Orchestrator model to both communicate and perform web browsing, but do not have it call the Consultant agent just yet. This approach allows you to test and debug the browsing functionality independently, ensuring it works correctly before integrating the Consultant into the workflow. Let's review the next steps:\n",
    "1. Test model endpoint.\n",
    "2. Setup Orchestrator agent without any browsing tool.\n",
    "3. Setup Exa MCP client to connect to the orchestrator.\n",
    "4. Add Exa MCP to the Orchestrator.\n",
    "5. Wrap the consultant agent we created as a tool and connect to the orchestrator. \n",
    "\n",
    "Before we start, let's test the model end-point we ran earlier and ensure we can connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Orchestrator (Qwen3-30B-A3B-Instruct-2507) endpoint\n",
    "!curl http://localhost:9001/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd046cd9",
   "metadata": {},
   "source": [
    "Time to setup the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c64fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "# Point to your Qwen3 endpoint (adjust if different port/model name)\n",
    "orch_provider = OpenAIProvider(base_url=\"http://localhost:9001/v1\", api_key=\"EMPTY\")\n",
    "orch_model = OpenAIChatModel(\"/models/Qwen3-30B-A3B-Instruct-2507\", provider=orch_provider)\n",
    "\n",
    "ORCH_SYS_PHASE1 = \"\"\"\n",
    "You are an Ingredient Orchestrator.\n",
    "Goal: When the user names a packaged snack, Return ONLY a cleaned ingredient list (bullet or comma-separated). \n",
    "If ambiguous, ask one clarifying question.\n",
    "Keep response ≤5 lines. Educational, not medical advice.\n",
    "\"\"\"\n",
    "\n",
    "orchestrator_phase1 = Agent(\n",
    "    model=orch_model,\n",
    "    system_prompt=ORCH_SYS_PHASE1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2939b",
   "metadata": {},
   "source": [
    "Let's test the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with orchestrator_phase1:\n",
    "    demo = await orchestrator_phase1.run(\"Ingredients for Kitkat (USA)?\")\n",
    "    print(demo.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973cd8d",
   "metadata": {},
   "source": [
    "### Step 7: Enable Web Search (Exa via MCP)\n",
    "\n",
    "To ensure the Orchestrator always provides accurate and up-to-date ingredient information, we’ll equip it with a browsing tool rather than relying on guesses or outdated data.\n",
    "\n",
    "Why use Exa with MCP?\n",
    "- MCP wraps remote capabilities as tools the model can call.\n",
    "- Exa provides fresh ingredient data directly from manufacturer and retailer websites.\n",
    "- By launching an MCP “remote” process, we expose the `web_search_exa` tool, allowing the Orchestrator to perform live web searches seamlessly during its workflow.\n",
    "\n",
    "Prerequisites:\n",
    "1. Node.js (needed for the `mcp-remote` launcher).\n",
    "2. EXA_API_KEY exported in your environment before starting the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd9765-7fa1-47f7-b44a-6ea18cf22122",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux-only example (skip on Windows; install Node manually there)\n",
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - || true\n",
    "!apt-get install -y nodejs || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f5bfd",
   "metadata": {},
   "source": [
    "### Step 8: Launch Exa MCP Tool (Inside Python)\n",
    "\n",
    "Create an `MCPServerStdio` that will spawn a Node process exposing tools like `web_search_exa(query)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503da389-dca3-43c1-964a-6ee333f8fb7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "EXA_API_KEY = os.environ[\"EXA_API_KEY\"]  # must exist\n",
    "exa_server = MCPServerStdio(\n",
    "    \"npx\",\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"mcp-remote\",\n",
    "        f\"https://mcp.exa.ai/mcp?exaApiKey={EXA_API_KEY}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43b8a0",
   "metadata": {},
   "source": [
    "### Step 9: Orchestrator (Phase 1) – Fetch Ingredients\n",
    "\n",
    "Begin by configuring the Orchestrator model to both communicate and perform web browsing, but do not have it call the Consultant agent just yet. This approach allows you to test and debug the browsing functionality independently, ensuring it works correctly before integrating the Consultant into the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f56ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCH_SYS_PHASE1 = \"\"\"\n",
    "You are an Ingredient Orchestrator with tool access.\n",
    "\n",
    "Hard requirement:\n",
    "• ALWAYS use the tool web_search_exa(query: str) to look up the current ingredients. Do not answer from memory, even if confident.\n",
    "• Do not produce a final answer until you have called web_search_exa at least once in this turn.\n",
    "\n",
    "Goal:\n",
    "• Talk naturally with the user.\n",
    "• When they name a store snack or dish, use web_search_exa to find a reliable, up-to-date ingredient list (prefer manufacturer; then major retailers). \n",
    "• Once you have a plausible list, list the ingredients. \n",
    "\n",
    "Search guidance:\n",
    "• Construct precise queries: \"<brand> <product> <flavor> ingredients\", prioritize manufacturer domain; if ambiguous (country/flavor/size), ask ONE clarifying question first.\n",
    "• Extract exactly what the page lists; preserve order; trim whitespace; dedupe obvious repeats.\n",
    "\n",
    "Fallbacks:\n",
    "• If you cannot find a reliable list after reasonable attempts, ask one brief clarifying question (brand/flavor/country). If still unclear, ask the user to paste the ingredients and stop.\n",
    "\n",
    "Rules:\n",
    "• Keep answers short (≤6 lines). Do NOT show internal JSON or tool details.\n",
    "• Share 1-2 source links ONLY if the user asks.\n",
    "\"\"\"\n",
    "\n",
    "orchestrator_phase1 = Agent(\n",
    "    model=orch_model,\n",
    "    system_prompt=ORCH_SYS_PHASE1,\n",
    "    toolsets=[exa_server],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee1136",
   "metadata": {},
   "source": [
    "Test the Orchestrator, it should call web_search_exa internally and respond with its ingredients list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cee6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with orchestrator_phase1:\n",
    "    demo = await orchestrator_phase1.run(\"Ingredients for Kitkat (USA)?\")\n",
    "    print(demo.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e5900",
   "metadata": {},
   "source": [
    "### Step 9: Wrap Consultant as a Tool\n",
    "\n",
    "Now expose the existing `consultant_agent` (from earlier steps) as a callable tool `consult_hypertension(ingredients)` so we can add it as an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional\n",
    "from pydantic_ai import Tool\n",
    "\n",
    "async def consult_hypertension_fn(\n",
    "    ingredients: List[str],\n",
    "    sodium_mg_per_serving: Optional[int] = None,\n",
    "    product_name: Optional[str] = None,\n",
    ") -> dict:\n",
    "    payload = {\"product_name\": product_name, \"ingredients\": ingredients}\n",
    "    if sodium_mg_per_serving is not None:\n",
    "        payload[\"nutrition\"] = {\"sodium_mg_per_serving\": sodium_mg_per_serving}\n",
    "\n",
    "    async with consultant_agent:\n",
    "        res = await consultant_agent.run(json.dumps(payload))\n",
    "\n",
    "    try:\n",
    "        return json.loads(res.output)\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"avoid\": [],\n",
    "            \"caution\": [],\n",
    "            \"overall\": \"uncertain\",\n",
    "            \"notes\": [\"consultant returned non-JSON\"],\n",
    "            \"disclaimer\": \"Educational only; not medical advice.\",\n",
    "        }\n",
    "\n",
    "consult_hypertension = Tool(\n",
    "    consult_hypertension_fn,\n",
    "    name=\"consult_hypertension\",\n",
    "    description=\"Evaluate ingredient list for high BP; returns JSON verdict.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b19fd",
   "metadata": {},
   "source": [
    "### Step 10: Orchestrator (Phase 2) – Enforcing Mandatory Search and Consultant Call\n",
    "To ensure the agents operate reliably and consistently, it’s important to clearly define the workflow and impose necessary restrictions on the Orchestrator when we add the Consultant.\n",
    "\n",
    "Our enforcement rules are:\n",
    "1. Always perform a `web_search_exa` call before finalizing any decisions. This guarantees the use of up-to-date ingredient information.\n",
    "1. After gathering the ingredient list, invoke the `consult_hypertension` agent to analyze potential health impacts.\n",
    "1. Provide a concise summarized verdict that categorizes the result as OK, Caution, or Avoid, along with 1 to 3 key reasons supporting the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCH_SYS_FINAL = \"\"\"\n",
    "You are an Ingredient Orchestrator with tool access.\n",
    "\n",
    "Hard requirement:\n",
    "• ALWAYS call web_search_exa(query) to obtain current ingredients (never answer from memory).\n",
    "• Do not give a verdict until you have performed at least one search this turn.\n",
    "\n",
    "Flow:\n",
    "1. Clarify brand/flavor/country only if ambiguous (ask one short question).\n",
    "2. Use web_search_exa to gather a reliable ingredient list (prefer manufacturer domain).\n",
    "3. Clean + dedupe; exclude allergen “contains” lines & marketing claims.\n",
    "4. Call consult_hypertension(ingredients[]) for verdict.\n",
    "5. Return concise answer: Verdict (OK / Caution / Avoid) + 1-3 concrete reason snippets (e.g., “high sodium”, “contains MSG”).\n",
    "\n",
    "Rules:\n",
    "• ≤6 lines total.\n",
    "• No raw tool JSON or internal traces.\n",
    "• Provide links only if user asks.\n",
    "• Educational, not medical advice.\n",
    "\"\"\"\n",
    "\n",
    "orchestrator = Agent(\n",
    "    model=orch_model,\n",
    "    system_prompt=ORCH_SYS_FINAL,\n",
    "    toolsets=[exa_server],\n",
    "    tools=[consult_hypertension],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565a485",
   "metadata": {},
   "source": [
    "Awesome, now you can do an end-to-end test that uses two agents (Orchestrator and Consultant) and two models (Qwen3-30B-A3B-Instruct-2507 and GPT-OSS-120B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69648ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with orchestrator:\n",
    "    result = await orchestrator.run(\"Is KitKat (US) okay for high blood pressure?\")\n",
    "    print(result.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
